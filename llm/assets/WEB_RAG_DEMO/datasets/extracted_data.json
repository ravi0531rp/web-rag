{
    "https://ravi0531rp.github.io/web-rag/services/satellite_images.html": [
        "Satellite Images",
        "Explore the world from above",
        "Discover the World with Satellite Images",
        "Satellite imagery offers a unique perspective of our planet, allowing us to explore landscapes, cities, and natural wonders from above. At ZeppLaud, we provide access to high-resolution satellite images that enable a variety of applications:",
        "Urban Planning: Analyze city layouts, infrastructure, and population density for effective urban planning. Environmental Monitoring: Track changes in vegetation, land use, and environmental conditions over time. Disaster Response: Assess damage and plan relief efforts during natural disasters such as floods, wildfires, and earthquakes. Navigation: Use satellite imagery for precise navigation, route planning, and location-based services. Scientific Research: Study geological features, climate patterns, and ecosystem dynamics for scientific research and exploration.",
        "Urban Planning: Analyze city layouts, infrastructure, and population density for effective urban planning.",
        "Environmental Monitoring: Track changes in vegetation, land use, and environmental conditions over time.",
        "Disaster Response: Assess damage and plan relief efforts during natural disasters such as floods, wildfires, and earthquakes.",
        "Navigation: Use satellite imagery for precise navigation, route planning, and location-based services.",
        "Scientific Research: Study geological features, climate patterns, and ecosystem dynamics for scientific research and exploration.",
        "Our satellite imagery is collected from a constellation of advanced Earth observation satellites equipped with state-of-the-art sensors. These satellites capture images with exceptional detail, allowing you to zoom in and explore areas with remarkable clarity.",
        "Key Features of Our Satellite Images:",
        "High-resolution imagery with up to sub-meter pixel resolution Global coverage with frequent revisit rates Multi-spectral bands for advanced analysis Historical archives for time-series analysis",
        "High-resolution imagery with up to sub-meter pixel resolution",
        "Global coverage with frequent revisit rates",
        "Multi-spectral bands for advanced analysis",
        "Historical archives for time-series analysis",
        "Whether you're a researcher, government agency, or commercial entity, our satellite imagery solutions provide valuable insights for a wide range of applications. Start exploring the world from a new perspective with ZeppLaud's satellite images.",
        "Note: The satellite image displayed above is for illustrative purposes only and may not represent the actual resolution or coverage of our imagery.",
        "\u00a9 2024 ZeppLaud. All rights reserved."
    ],
    "https://ravi0531rp.github.io/web-rag/index.html": [
        "Welcome to ZeppLaud",
        "Your partner in satellite imagery solutions",
        "Our Services",
        "We specialize in providing advanced satellite imagery software solutions, including:",
        "Satellite Images AI Models for Image Analysis Drone Imagery Solutions",
        "Satellite Images",
        "AI Models for Image Analysis",
        "Drone Imagery Solutions",
        "How We Do It",
        "At ZeppLaud, we leverage cutting-edge technology and expertise to deliver top-notch satellite imagery solutions. Our process involves:",
        "Advanced Data Collection Techniques Machine Learning Algorithms Customized Software Development",
        "Advanced Data Collection Techniques",
        "Machine Learning Algorithms",
        "Customized Software Development",
        "Our Products",
        "Satellite Images",
        "AI Models",
        "Drone Imagery",
        "Words from Our Clients",
        "\u00a9 2024 ZeppLaud. All rights reserved."
    ],
    "https://ravi0531rp.github.io/web-rag/services/drone_imagery.html": [
        "Drone Imagery Services",
        "Unlocking aerial perspectives for actionable insights",
        "Our Services",
        "Drone Imagery Acquisition",
        "Our team utilizes advanced drone technology to capture high-resolution imagery from various altitudes and angles, providing comprehensive coverage of the designated area.",
        "Image Processing and Analysis",
        "We employ state-of-the-art image processing techniques to enhance, analyze, and extract valuable insights from the drone-acquired imagery, enabling informed decision-making.",
        "Customized Solutions",
        "From precision agriculture to infrastructure inspection, we tailor our drone imagery services to meet the specific needs and requirements of each client, ensuring maximum efficiency and effectiveness.",
        "Benefits",
        "Highly detailed aerial views Cost-effective and efficient data collection Timely and accurate analysis Enhanced safety and accessibility Customizable solutions for diverse applications",
        "Highly detailed aerial views",
        "Cost-effective and efficient data collection",
        "Timely and accurate analysis",
        "Enhanced safety and accessibility",
        "Customizable solutions for diverse applications",
        "\u00a9 2024 ZeppLaud. All rights reserved."
    ],
    "https://ravi0531rp.github.io/web-rag/services/ai_models.html": [
        "AI Models",
        "Empowering image-based AI solutions",
        "Classification Models",
        "Model 1: Object Detection Classifier ConvNext V2",
        "Our Object Detection Classifier identifies and localizes multiple objects within an image. It's useful for tasks such as counting objects in a scene, detecting anomalies, and monitoring inventory.",
        "Our Object Detection Classifier stands as a cornerstone in the realm of computer vision, empowering businesses and researchers alike with its ability to discern and precisely locate multiple objects within complex images. Beyond mere recognition, it excels in understanding spatial relationships, enabling applications ranging from crowd analysis in urban settings to wildlife monitoring in expansive natural habitats.",
        "Leveraging cutting-edge deep learning algorithms, our Object Detection Classifier transcends traditional image recognition approaches by not only identifying objects but also providing granular insights into their contextual significance. This capability finds extensive utility in domains such as retail, where it aids in shelf monitoring, product placement optimization, and even customer behavior analysis, enhancing operational efficiency and customer satisfaction simultaneously.",
        "The versatility of our Object Detection Classifier extends far beyond conventional use cases, delving into niche applications that demand precision and adaptability. From medical imaging, where it assists in tumor detection and diagnosis, to autonomous vehicles, where it ensures safe navigation by recognizing road signs and obstacles in real-time, its impact reverberates across diverse sectors, reshaping industries and driving innovation forward.",
        "Metric Value Accuracy 0.92 Precision 0.88 Recall 0.94 F1 Score 0.91",
        "Model 2: Facial Expression Recognition Classifier",
        "Our Facial Expression Recognition Classifier analyzes facial expressions in images, enabling emotion detection and sentiment analysis. It's valuable for applications such as customer feedback analysis, human-computer interaction, and market research.",
        "In addition to its primary applications, our Facial Expression Recognition Classifier plays a pivotal role in enhancing user experience in various digital platforms. By accurately capturing and interpreting facial expressions, it facilitates personalized content delivery, adaptive user interfaces, and immersive virtual experiences, thereby fostering deeper engagement and satisfaction.",
        "Furthermore, the Facial Expression Recognition Classifier serves as a powerful tool in healthcare settings, aiding medical professionals in assessing patients' emotional states and mental well-being remotely. Its non-invasive nature and real-time analysis capabilities make it an invaluable asset in telemedicine, therapy sessions, and clinical research, augmenting diagnostic accuracy and treatment effectiveness.",
        "The versatility and accuracy of our Facial Expression Recognition Classifier make it an indispensable component in the ever-evolving landscape of AI-driven technologies, empowering businesses, healthcare providers, and researchers to unlock new possibilities and create meaningful human-machine interactions.",
        "Metric Value Accuracy 0.87 Precision 0.85 Recall 0.89 F1 Score 0.86",
        "Model 3: Scene Recognition Classifier",
        "Our Scene Recognition Classifier categorizes images into various scenes and environments, providing valuable context for image understanding and content-based retrieval. It's essential for applications such as image search, automated tagging, and content recommendation.",
        "Metric Value Accuracy 0.94 Precision 0.91 Recall 0.95 F1 Score 0.93",
        "Segmentation Models",
        "Regression Models",
        "\u00a9 2024 ZeppLaud. All rights reserved."
    ],
    "https://huggingface.co/docs/transformers/en/model_doc/convnextv2": [
        "Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up",
        "Models",
        "Datasets",
        "Spaces",
        "Posts",
        "Docs",
        "Solutions",
        "Pricing",
        "Log In",
        "Sign Up",
        "Transformers documentation",
        "ConvNeXt V2",
        "Transformers",
        "and get access to the augmented documentation experience",
        "to get started",
        "ConvNeXt V2",
        "Overview",
        "The ConvNeXt V2 model was proposed in ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\nConvNeXt V2 is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, and a successor of ConvNeXT .",
        "The abstract from the paper is the following:",
        "Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked  autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data.",
        "This model was contributed by adirik . The original code can be found here .",
        "Resources",
        "A list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ConvNeXt V2.",
        "ConvNextV2ForImageClassification is supported by this example script and notebook .",
        "ConvNextV2ForImageClassification is supported by this example script and notebook .",
        "If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",
        "ConvNextV2Config",
        "class transformers. ConvNextV2Config",
        "( num_channels = 3 patch_size = 4 num_stages = 4 hidden_sizes = None depths = None hidden_act = 'gelu' initializer_range = 0.02 layer_norm_eps = 1e-12 drop_path_rate = 0.0 image_size = 224 out_features = None out_indices = None **kwargs )",
        "Parameters",
        "num_channels ( int , optional , defaults to 3) \u2014\nThe number of input channels. patch_size ( int , optional, defaults to 4) \u2014\nPatch size to use in the patch embedding layer. num_stages ( int , optional, defaults to 4) \u2014\nThe number of stages in the model. hidden_sizes ( List[int] , optional , defaults to [96, 192, 384, 768] ) \u2014\nDimensionality (hidden size) at each stage. depths ( List[int] , optional , defaults to [3, 3, 9, 3] ) \u2014\nDepth (number of blocks) for each stage. hidden_act ( str or function , optional , defaults to \"gelu\" ) \u2014\nThe non-linear activation function (function or string) in each block. If string, \"gelu\" , \"relu\" , \"selu\" and \"gelu_new\" are supported. initializer_range ( float , optional , defaults to 0.02) \u2014\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices. layer_norm_eps ( float , optional , defaults to 1e-12) \u2014\nThe epsilon used by the layer normalization layers. drop_path_rate ( float , optional , defaults to 0.0) \u2014\nThe drop rate for stochastic depth. out_features ( List[str] , optional ) \u2014\nIf used as backbone, list of features to output. Can be any of \"stem\" , \"stage1\" , \"stage2\" , etc.\n(depending on how many stages the model has). If unset and out_indices is set, will default to the\ncorresponding stages. If unset and out_indices is unset, will default to the last stage. Must be in the\nsame order as defined in the stage_names attribute. out_indices ( List[int] , optional ) \u2014\nIf used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\nmany stages the model has). If unset and out_features is set, will default to the corresponding stages.\nIf unset and out_features is unset, will default to the last stage. Must be in the\nsame order as defined in the stage_names attribute.",
        "num_channels ( int , optional , defaults to 3) \u2014\nThe number of input channels.",
        "patch_size ( int , optional, defaults to 4) \u2014\nPatch size to use in the patch embedding layer.",
        "num_stages ( int , optional, defaults to 4) \u2014\nThe number of stages in the model.",
        "hidden_sizes ( List[int] , optional , defaults to [96, 192, 384, 768] ) \u2014\nDimensionality (hidden size) at each stage.",
        "depths ( List[int] , optional , defaults to [3, 3, 9, 3] ) \u2014\nDepth (number of blocks) for each stage.",
        "hidden_act ( str or function , optional , defaults to \"gelu\" ) \u2014\nThe non-linear activation function (function or string) in each block. If string, \"gelu\" , \"relu\" , \"selu\" and \"gelu_new\" are supported.",
        "initializer_range ( float , optional , defaults to 0.02) \u2014\nThe standard deviation of the truncated_normal_initializer for initializing all weight matrices.",
        "layer_norm_eps ( float , optional , defaults to 1e-12) \u2014\nThe epsilon used by the layer normalization layers.",
        "drop_path_rate ( float , optional , defaults to 0.0) \u2014\nThe drop rate for stochastic depth.",
        "out_features ( List[str] , optional ) \u2014\nIf used as backbone, list of features to output. Can be any of \"stem\" , \"stage1\" , \"stage2\" , etc.\n(depending on how many stages the model has). If unset and out_indices is set, will default to the\ncorresponding stages. If unset and out_indices is unset, will default to the last stage. Must be in the\nsame order as defined in the stage_names attribute.",
        "out_indices ( List[int] , optional ) \u2014\nIf used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\nmany stages the model has). If unset and out_features is set, will default to the corresponding stages.\nIf unset and out_features is unset, will default to the last stage. Must be in the\nsame order as defined in the stage_names attribute.",
        "This is the configuration class to store the configuration of a ConvNextV2Model . It is used to instantiate an\nConvNeXTV2 model according to the specified arguments, defining the model architecture. Instantiating a\nconfiguration with the defaults will yield a similar configuration to that of the ConvNeXTV2 facebook/convnextv2-tiny-1k-224 architecture.",
        "Configuration objects inherit from PretrainedConfig and can be used to control the model outputs. Read the\ndocumentation from PretrainedConfig for more information.",
        "Example:",
        "ConvNextV2Model",
        "class transformers. ConvNextV2Model",
        "( config )",
        "Parameters",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "The bare ConvNextV2 model outputting raw features without any specific head on top.\nThis model is a PyTorch torch.nn.Module subclass. Use it\nas a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\nbehavior.",
        "forward",
        "( pixel_values : FloatTensor = None output_hidden_states : Optional = None return_dict : Optional = None ) \u2192 transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)",
        "Parameters",
        "pixel_values ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using ConvNextImageProcessor . See ConvNextImageProcessor. call () for details. output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple.",
        "pixel_values ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using ConvNextImageProcessor . See ConvNextImageProcessor. call () for details.",
        "output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail.",
        "return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple.",
        "Returns",
        "transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or tuple(torch.FloatTensor)",
        "A transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or a tuple of torch.FloatTensor (if return_dict=False is passed or when config.return_dict=False ) comprising various\nelements depending on the configuration ( ConvNextV2Config ) and inputs. last_hidden_state ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model. pooler_output ( torch.FloatTensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions. hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "A transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention or a tuple of torch.FloatTensor (if return_dict=False is passed or when config.return_dict=False ) comprising various\nelements depending on the configuration ( ConvNextV2Config ) and inputs.",
        "last_hidden_state ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model. pooler_output ( torch.FloatTensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions. hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "last_hidden_state ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model.",
        "last_hidden_state ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model.",
        "pooler_output ( torch.FloatTensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.",
        "pooler_output ( torch.FloatTensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.",
        "hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each layer) of shape (batch_size, num_channels, height, width) .",
        "Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "The ConvNextV2Model forward method, overrides the __call__ special method.",
        "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.",
        "Example:",
        "ConvNextV2ForImageClassification",
        "class transformers. ConvNextV2ForImageClassification",
        "( config )",
        "Parameters",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\nImageNet.",
        "This model is a PyTorch torch.nn.Module subclass. Use it\nas a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\nbehavior.",
        "forward",
        "( pixel_values : FloatTensor = None labels : Optional = None output_hidden_states : Optional = None return_dict : Optional = None ) \u2192 transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)",
        "Parameters",
        "pixel_values ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using ConvNextImageProcessor . See ConvNextImageProcessor. call () for details. output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple. labels ( torch.LongTensor of shape (batch_size,) , optional ) \u2014\nLabels for computing the image classification/regression loss. Indices should be in [0, ..., config.num_labels - 1] . If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels > 1 a classification loss is computed (Cross-Entropy).",
        "pixel_values ( torch.FloatTensor of shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using ConvNextImageProcessor . See ConvNextImageProcessor. call () for details.",
        "output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail.",
        "return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple.",
        "labels ( torch.LongTensor of shape (batch_size,) , optional ) \u2014\nLabels for computing the image classification/regression loss. Indices should be in [0, ..., config.num_labels - 1] . If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels > 1 a classification loss is computed (Cross-Entropy).",
        "Returns",
        "transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or tuple(torch.FloatTensor)",
        "A transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or a tuple of torch.FloatTensor (if return_dict=False is passed or when config.return_dict=False ) comprising various\nelements depending on the configuration ( ConvNextV2Config ) and inputs. loss ( torch.FloatTensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss. logits ( torch.FloatTensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax). hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also\ncalled feature maps) of the model at the output of each stage.",
        "A transformers.modeling_outputs.ImageClassifierOutputWithNoAttention or a tuple of torch.FloatTensor (if return_dict=False is passed or when config.return_dict=False ) comprising various\nelements depending on the configuration ( ConvNextV2Config ) and inputs.",
        "loss ( torch.FloatTensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss. logits ( torch.FloatTensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax). hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also\ncalled feature maps) of the model at the output of each stage.",
        "loss ( torch.FloatTensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss.",
        "logits ( torch.FloatTensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).",
        "hidden_states ( tuple(torch.FloatTensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, +\none for the output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also\ncalled feature maps) of the model at the output of each stage.",
        "The ConvNextV2ForImageClassification forward method, overrides the __call__ special method.",
        "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.",
        "Example:",
        "TFConvNextV2Model",
        "class transformers. TFConvNextV2Model",
        "( config : ConvNextV2Config *inputs **kwargs )",
        "Parameters",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "The bare ConvNextV2 model outputting raw features without any specific head on top.\nThis model inherits from TFPreTrainedModel . Check the superclass documentation for the generic methods the\nlibrary implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\netc.)",
        "This model is also a keras.Model subclass. Use it\nas a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and\nbehavior.",
        "TensorFlow models and layers in transformers accept two formats as input:",
        "having all inputs as keyword arguments (like PyTorch models), or having all inputs as a list, tuple or dict in the first positional argument.",
        "having all inputs as keyword arguments (like PyTorch models), or",
        "having all inputs as a list, tuple or dict in the first positional argument.",
        "The reason the second format is supported is that Keras methods prefer this format when passing inputs to models\nand layers. Because of this support, when using methods like model.fit() things should \u201cjust work\u201d for you - just\npass your inputs and labels in any format that model.fit() supports! If, however, you want to use the second\nformat outside of Keras methods like fit() and predict() , such as when creating your own layers or models with\nthe Keras Functional API, there are three possibilities you can use to gather all the input Tensors in the first\npositional argument:",
        "a single Tensor with pixel_values only and nothing else: model(pixel_values) a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([pixel_values, attention_mask]) or model([pixel_values, attention_mask, token_type_ids]) a dictionary with one or several input Tensors associated to the input names given in the docstring: model({\"pixel_values\": pixel_values, \"token_type_ids\": token_type_ids})",
        "a single Tensor with pixel_values only and nothing else: model(pixel_values)",
        "a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([pixel_values, attention_mask]) or model([pixel_values, attention_mask, token_type_ids])",
        "a dictionary with one or several input Tensors associated to the input names given in the docstring: model({\"pixel_values\": pixel_values, \"token_type_ids\": token_type_ids})",
        "Note that when creating models and layers with subclassing then you don\u2019t need to worry\nabout any of this, as you can just pass inputs like you would to any other Python function!",
        "call",
        "( pixel_values : TFModelInputType | None = None output_hidden_states : Optional[bool] = None return_dict : Optional[bool] = None training : bool = False ) \u2192 transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention or tuple(tf.Tensor)",
        "Parameters",
        "pixel_values ( np.ndarray , tf.Tensor , List[tf.Tensor] , Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each example must have the shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using AutoImageProcessor . See ConvNextImageProcessor. call () for details. output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. This argument can be used only in eager mode, in graph mode the value in the config will be\nused instead. return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple. This argument can be used in\neager mode, in graph mode the value will always be set to True .",
        "pixel_values ( np.ndarray , tf.Tensor , List[tf.Tensor] , Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each example must have the shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using AutoImageProcessor . See ConvNextImageProcessor. call () for details.",
        "output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. This argument can be used only in eager mode, in graph mode the value in the config will be\nused instead.",
        "return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple. This argument can be used in\neager mode, in graph mode the value will always be set to True .",
        "Returns",
        "transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention or tuple(tf.Tensor)",
        "A transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention or a tuple of tf.Tensor (if return_dict=False is passed or when config.return_dict=False ) comprising various elements depending on the\nconfiguration ( ConvNextV2Config ) and inputs. last_hidden_state ( tf.Tensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model. pooler_output ( tf.Tensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions. hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "A transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention or a tuple of tf.Tensor (if return_dict=False is passed or when config.return_dict=False ) comprising various elements depending on the\nconfiguration ( ConvNextV2Config ) and inputs.",
        "last_hidden_state ( tf.Tensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model. pooler_output ( tf.Tensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions. hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "last_hidden_state ( tf.Tensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model.",
        "last_hidden_state ( tf.Tensor of shape (batch_size, num_channels, height, width) ) \u2014 Sequence of hidden-states at the output of the last layer of the model.",
        "pooler_output ( tf.Tensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.",
        "pooler_output ( tf.Tensor of shape (batch_size, hidden_size) ) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.",
        "hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each layer) of shape (batch_size, num_channels, height, width) . Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each layer) of shape (batch_size, num_channels, height, width) .",
        "Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.",
        "The TFConvNextV2Model forward method, overrides the __call__ special method.",
        "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.",
        "Example:",
        "TFConvNextV2ForImageClassification",
        "class transformers. TFConvNextV2ForImageClassification",
        "( config : ConvNextV2Config *inputs **kwargs )",
        "Parameters",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "config ( ConvNextV2Config ) \u2014 Model configuration class with all the parameters of the model.\nInitializing with a config file does not load the weights associated with the model, only the\nconfiguration. Check out the from_pretrained() method to load the model weights.",
        "ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\nImageNet.",
        "This model inherits from TFPreTrainedModel . Check the superclass documentation for the generic methods the\nlibrary implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\netc.)",
        "This model is also a keras.Model subclass. Use it\nas a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and\nbehavior.",
        "TensorFlow models and layers in transformers accept two formats as input:",
        "having all inputs as keyword arguments (like PyTorch models), or having all inputs as a list, tuple or dict in the first positional argument.",
        "having all inputs as keyword arguments (like PyTorch models), or",
        "having all inputs as a list, tuple or dict in the first positional argument.",
        "The reason the second format is supported is that Keras methods prefer this format when passing inputs to models\nand layers. Because of this support, when using methods like model.fit() things should \u201cjust work\u201d for you - just\npass your inputs and labels in any format that model.fit() supports! If, however, you want to use the second\nformat outside of Keras methods like fit() and predict() , such as when creating your own layers or models with\nthe Keras Functional API, there are three possibilities you can use to gather all the input Tensors in the first\npositional argument:",
        "a single Tensor with pixel_values only and nothing else: model(pixel_values) a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([pixel_values, attention_mask]) or model([pixel_values, attention_mask, token_type_ids]) a dictionary with one or several input Tensors associated to the input names given in the docstring: model({\"pixel_values\": pixel_values, \"token_type_ids\": token_type_ids})",
        "a single Tensor with pixel_values only and nothing else: model(pixel_values)",
        "a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([pixel_values, attention_mask]) or model([pixel_values, attention_mask, token_type_ids])",
        "a dictionary with one or several input Tensors associated to the input names given in the docstring: model({\"pixel_values\": pixel_values, \"token_type_ids\": token_type_ids})",
        "Note that when creating models and layers with subclassing then you don\u2019t need to worry\nabout any of this, as you can just pass inputs like you would to any other Python function!",
        "call",
        "( pixel_values : TFModelInputType | None = None output_hidden_states : Optional[bool] = None return_dict : Optional[bool] = None labels : np.ndarray | tf.Tensor | None = None training : Optional[bool] = False ) \u2192 transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)",
        "Parameters",
        "pixel_values ( np.ndarray , tf.Tensor , List[tf.Tensor] , Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each example must have the shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using AutoImageProcessor . See ConvNextImageProcessor. call () for details. output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. This argument can be used only in eager mode, in graph mode the value in the config will be\nused instead. return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple. This argument can be used in\neager mode, in graph mode the value will always be set to True . labels ( tf.Tensor or np.ndarray of shape (batch_size,) , optional ) \u2014\nLabels for computing the image classification/regression loss. Indices should be in [0, ..., config.num_labels - 1] . If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels > 1 a classification loss is computed (Cross-Entropy).",
        "pixel_values ( np.ndarray , tf.Tensor , List[tf.Tensor] , Dict[str, tf.Tensor] or Dict[str, np.ndarray] and each example must have the shape (batch_size, num_channels, height, width) ) \u2014\nPixel values. Pixel values can be obtained using AutoImageProcessor . See ConvNextImageProcessor. call () for details.",
        "output_hidden_states ( bool , optional ) \u2014\nWhether or not to return the hidden states of all layers. See hidden_states under returned tensors for\nmore detail. This argument can be used only in eager mode, in graph mode the value in the config will be\nused instead.",
        "return_dict ( bool , optional ) \u2014\nWhether or not to return a ModelOutput instead of a plain tuple. This argument can be used in\neager mode, in graph mode the value will always be set to True .",
        "labels ( tf.Tensor or np.ndarray of shape (batch_size,) , optional ) \u2014\nLabels for computing the image classification/regression loss. Indices should be in [0, ..., config.num_labels - 1] . If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels > 1 a classification loss is computed (Cross-Entropy).",
        "Returns",
        "transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or tuple(tf.Tensor)",
        "A transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or a tuple of tf.Tensor (if return_dict=False is passed or when config.return_dict=False ) comprising various elements depending on the\nconfiguration ( ConvNextV2Config ) and inputs. loss ( tf.Tensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss. logits ( tf.Tensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax). hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also called\nfeature maps) of the model at the output of each stage.",
        "A transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention or a tuple of tf.Tensor (if return_dict=False is passed or when config.return_dict=False ) comprising various elements depending on the\nconfiguration ( ConvNextV2Config ) and inputs.",
        "loss ( tf.Tensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss. logits ( tf.Tensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax). hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also called\nfeature maps) of the model at the output of each stage.",
        "loss ( tf.Tensor of shape (1,) , optional , returned when labels is provided) \u2014 Classification (or regression if config.num_labels==1) loss.",
        "logits ( tf.Tensor of shape (batch_size, config.num_labels) ) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).",
        "hidden_states ( tuple(tf.Tensor) , optional , returned when output_hidden_states=True is passed or when config.output_hidden_states=True ) \u2014 Tuple of tf.Tensor (one for the output of the embeddings, if the model has an embedding layer, + one for\nthe output of each stage) of shape (batch_size, num_channels, height, width) . Hidden-states (also called\nfeature maps) of the model at the output of each stage.",
        "The TFConvNextV2ForImageClassification forward method, overrides the __call__ special method.",
        "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the pre and post processing steps while\nthe latter silently ignores them.",
        "Example:"
    ]
}